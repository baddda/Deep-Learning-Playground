{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import boston_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.23247e+00 0.00000e+00 8.14000e+00 ... 2.10000e+01 3.96900e+02\n",
      "  1.87200e+01]\n",
      " [2.17700e-02 8.25000e+01 2.03000e+00 ... 1.47000e+01 3.95380e+02\n",
      "  3.11000e+00]\n",
      " [4.89822e+00 0.00000e+00 1.81000e+01 ... 2.02000e+01 3.75520e+02\n",
      "  3.26000e+00]\n",
      " ...\n",
      " [3.46600e-02 3.50000e+01 6.06000e+00 ... 1.69000e+01 3.62250e+02\n",
      "  7.83000e+00]\n",
      " [2.14918e+00 0.00000e+00 1.95800e+01 ... 1.47000e+01 2.61950e+02\n",
      "  1.57900e+01]\n",
      " [1.43900e-02 6.00000e+01 2.93000e+00 ... 1.56000e+01 3.76700e+02\n",
      "  4.38000e+00]]\n",
      "[[-0.27224633 -0.48361547 -0.43576161 ...  1.14850044  0.44807713\n",
      "   0.8252202 ]\n",
      " [-0.40342651  2.99178419 -1.33391162 ... -1.71818909  0.43190599\n",
      "  -1.32920239]\n",
      " [ 0.1249402  -0.48361547  1.0283258  ...  0.78447637  0.22061726\n",
      "  -1.30850006]\n",
      " ...\n",
      " [-0.40202987  0.99079651 -0.7415148  ... -0.71712291  0.07943894\n",
      "  -0.67776904]\n",
      " [-0.17292018 -0.48361547  1.24588095 ... -1.71818909 -0.98764362\n",
      "   0.42083466]\n",
      " [-0.40422614  2.04394792 -1.20161456 ... -1.30866202  0.23317118\n",
      "  -1.15392266]]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "print(train_data)\n",
    "# \"you subtract the mean of the feature and divide by the standard deviation, so that the feature is centered around 0 and has a unit standard deviation\"\n",
    "# Called z score.\n",
    "\n",
    "# >>> a = np.array([[1, 2], [3, 4]])\n",
    "# >>> np.mean(a, axis=0)   -> Calculates mean.\n",
    "# array([2., 3.])          -> (1+3)/2\n",
    "# >>> np.std(a, axis=0)    -> Calculates standard deviation of axis 0.\n",
    "# array([1.,  1.])         -> ... TODO: add exact calculation example\n",
    "#\n",
    "# We should normalize our data, otherwise it is going to be hard for the network to learn. \n",
    "# Since some values of the input layer are much larger then others and therefore its harder for the model to adjust the weights to keep the wide spread in \"mind\".\n",
    "# We use here https://en.wikipedia.org/wiki/Standard_score\n",
    "# To archive this we need to aim for 1. and 2. (See further below)\n",
    "\n",
    "# 1. Goal is to have a \"expected value\" of 0. https://en.wikipedia.org/wiki/Expected_value\n",
    "# So basically this will move the x center of input deviation graph to the 0 for all data arrays.\n",
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "# 2. Goal is to have a \"variance\" of 1. https://en.wikipedia.org/wiki/Variance\n",
    "# So basically this will normalize the X axis of the input deviation graph.\n",
    "# X axis of the input deviation graph will look then like ...-2σ -1σ 0σ 1σ 2σ... σ = standard deviation for all data arrays\n",
    "std = train_data.std(axis=0) \n",
    "train_data /= std\n",
    "\n",
    "test_data -= mean \n",
    "test_data /= std\n",
    "print(train_data)\n",
    "print(train_data.std(axis=0)) # As you see its all 1 (Goal 2.)\n",
    "\n",
    "# Basically we have at the end for every data array the same input model (scale) in form of an array of x, where x represents with x*σ+mean the old value. (σ and mean is different for each axis)\n",
    "# \"normalization helps the backpropagation algorithm converge faster\"\n",
    "# To compare this with the MNIST example.\n",
    "# Y axis of the input deviation graph is count.\n",
    "# X axis of the input deviation graph is the gray scale.\n",
    "\n",
    "# Q: Can't we just map all axis into an range of [-1,+1]? (Min-Max method)?\n",
    "# A: Yes, but min max method is better for non bell shaped distribution, like MNIST. Because TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_python_3_6",
   "language": "python",
   "name": "env_python_3_6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
