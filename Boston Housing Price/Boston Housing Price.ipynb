{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import boston_housing\n",
    "import numpy as np\n",
    "from keras import optimizers\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.23247e+00 0.00000e+00 8.14000e+00 ... 2.10000e+01 3.96900e+02\n",
      "  1.87200e+01]\n",
      " [2.17700e-02 8.25000e+01 2.03000e+00 ... 1.47000e+01 3.95380e+02\n",
      "  3.11000e+00]\n",
      " [4.89822e+00 0.00000e+00 1.81000e+01 ... 2.02000e+01 3.75520e+02\n",
      "  3.26000e+00]\n",
      " ...\n",
      " [3.46600e-02 3.50000e+01 6.06000e+00 ... 1.69000e+01 3.62250e+02\n",
      "  7.83000e+00]\n",
      " [2.14918e+00 0.00000e+00 1.95800e+01 ... 1.47000e+01 2.61950e+02\n",
      "  1.57900e+01]\n",
      " [1.43900e-02 6.00000e+01 2.93000e+00 ... 1.56000e+01 3.76700e+02\n",
      "  4.38000e+00]]\n",
      "[[-0.27224633 -0.48361547 -0.43576161 ...  1.14850044  0.44807713\n",
      "   0.8252202 ]\n",
      " [-0.40342651  2.99178419 -1.33391162 ... -1.71818909  0.43190599\n",
      "  -1.32920239]\n",
      " [ 0.1249402  -0.48361547  1.0283258  ...  0.78447637  0.22061726\n",
      "  -1.30850006]\n",
      " ...\n",
      " [-0.40202987  0.99079651 -0.7415148  ... -0.71712291  0.07943894\n",
      "  -0.67776904]\n",
      " [-0.17292018 -0.48361547  1.24588095 ... -1.71818909 -0.98764362\n",
      "   0.42083466]\n",
      " [-0.40422614  2.04394792 -1.20161456 ... -1.30866202  0.23317118\n",
      "  -1.15392266]]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
    "print(train_data)\n",
    "# \"you subtract the mean of the feature and divide by the standard deviation, so that the feature is centered around 0 and has a unit standard deviation\"\n",
    "# Called z score.\n",
    "\n",
    "# >>> a = np.array([[1, 2], [3, 4]])\n",
    "# >>> np.mean(a, axis=0)   -> Calculates mean.\n",
    "# array([2., 3.])          -> (1+3)/2\n",
    "# >>> np.std(a, axis=0)    -> Calculates standard deviation of axis 0.\n",
    "# array([1.,  1.])         -> ... TODO: add exact calculation example\n",
    "#\n",
    "# We should normalize our data, otherwise it is going to be hard for the network to learn. \n",
    "# Since some values of the input layer are much larger then others and therefore its harder for the model to adjust the weights to keep the wide spread in \"mind\".\n",
    "# We use here https://en.wikipedia.org/wiki/Standard_score\n",
    "# To archive this we need to aim for 1. and 2. (See further below)\n",
    "\n",
    "# 1. Goal is to have a \"expected value\" of 0. https://en.wikipedia.org/wiki/Expected_value\n",
    "# So basically this will move the x center of input deviation graph to the 0 for all data arrays.\n",
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "# 2. Goal is to have a \"variance\" of 1. https://en.wikipedia.org/wiki/Variance\n",
    "# So basically this will normalize the X axis of the input deviation graph.\n",
    "# X axis of the input deviation graph will look then like ...-2σ -1σ 0σ 1σ 2σ... σ = standard deviation for all data arrays\n",
    "std = train_data.std(axis=0) \n",
    "train_data /= std\n",
    "\n",
    "test_data -= mean \n",
    "test_data /= std\n",
    "print(train_data)\n",
    "print(train_data.std(axis=0)) # As you see its all 1 (Goal 2.)\n",
    "\n",
    "# Basically we have at the end for every data array the same input model (scale) in form of an array of x, where x represents with x*σ+mean the old value. (σ and mean is different for each axis)\n",
    "# \"normalization helps the backpropagation algorithm converge faster\"\n",
    "# To compare this with the MNIST example.\n",
    "# Y axis of the input deviation graph is count.\n",
    "# X axis of the input deviation graph is the gray scale.\n",
    "\n",
    "# Q: Can't we just map all axis into an range of [-1,+1]? (Min-Max method)?\n",
    "# A: Yes, but min max method is better for non bell shaped distribution, like MNIST. Because TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential() \n",
    "    model.add(layers.Dense(64, activation='relu',input_shape=(train_data.shape[1],))) \n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'batch_index' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-af5811002d15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     history = model.fit(partial_train_data, partial_train_targets,\n\u001b[1;32m     26\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                         epochs=num_epochs, batch_size=1, verbose=0)\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mmae_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_mean_absolute_error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mall_mae_histories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmae_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Kera/env_python_3_6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/Kera/env_python_3_6/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Last batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdo_validation\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshould_run_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                     val_outs = test_loop(model, val_function, val_inputs,\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'batch_index' referenced before assignment"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "num_val_samples = len(train_data)\n",
    "num_epochs = 500\n",
    "all_mae_histories = []\n",
    "all_scores = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Build the Keras model (already compiled)\n",
    "    model = build_model()\n",
    "    # Train the model (in silent mode, verbose=0)\n",
    "    history = model.fit(partial_train_data, partial_train_targets,\n",
    "                        validation_data=(val_data, val_targets),\n",
    "                        epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    mae_history = history.history['val_mean_absolute_error']\n",
    "    all_mae_histories.append(mae_history)\n",
    "    \n",
    "average_mae_history = [\n",
    "    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
    "\n",
    "# TODO: fix compile error and finish from page 89 to 91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_python_3_6",
   "language": "python",
   "name": "env_python_3_6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
