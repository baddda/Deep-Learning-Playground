{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Based on https://blog.floydhub.com/spinning-up-with-deep-reinforcement-learning/\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from datetime import datetime\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect a game of Pong with random input.\n",
    "frames = []\n",
    "frames_ram = []\n",
    "STEPS = 300\n",
    "\n",
    "UP_ACTION = 2\n",
    "DOWN_ACTION = 3\n",
    "\n",
    "env = gym.make(\"Pong-ram-v0\")\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "for i in range(STEPS):\n",
    "    gym.envs.registry\n",
    "    action = random.randint(UP_ACTION, DOWN_ACTION)\n",
    "\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    frames.append(env.unwrapped._get_image())\n",
    "    frames_ram.append(env.unwrapped._get_ram())\n",
    "    \n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "        frames.append(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[192   0   0   0 110  38   0   7  97   1   0  33   0   0   0  63 255   0\n",
      " 255 253   0   0   0  24 128  32   1  86 247  86 247  86 247 134 243 245\n",
      " 243 240 240 242 242  32  32  64  64  64 188  65 189   0   0 164  37  37\n",
      "   0   0   0   0   0 109 164  37  37 192 192 192 192   1 192 202 247 202\n",
      " 247 202 247 202 247   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  54 236 242\n",
      " 121 240]\n",
      "Player position y: 164\n",
      "Ball position x: 0\n",
      "Ball position y: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-aea6d964cda5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ball position y: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes_ram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mram_address_position_ball_y\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Draw our frames to see what happened and print ram to find important addresses.\n",
    "ram_address_position_player_y = 60\n",
    "ram_address_position_ball_x = 49\n",
    "ram_address_position_ball_y = 50\n",
    "\n",
    "for i in range(len(frames)):\n",
    "    plt.imshow(frames[i])\n",
    "    print(frames_ram[i])\n",
    "    print('Player position y: ' + str(frames_ram[i][ram_address_position_player_y]))\n",
    "    print('Ball position x: ' + str(frames_ram[i][ram_address_position_ball_x]))\n",
    "    print('Ball position y: ' + str(frames_ram[i][ram_address_position_ball_y]))\n",
    "    display.display(pl.gcf())\n",
    "    time.sleep(0.5)\n",
    "    display.clear_output(wait=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the neural network.\n",
    "# TODO: What is a adam optimizer?\n",
    "model = Sequential()\n",
    "model.add(Dense(units=200,input_dim=3, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "model.add(Dense(units=1, activation='sigmoid', kernel_initializer='RandomNormal'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# gym initialization\n",
    "env = gym.make(\"Pong-ram-v0\")\n",
    "observation = env.reset()\n",
    "prev_input = None\n",
    "\n",
    "# Macros\n",
    "UP_ACTION = 2\n",
    "DOWN_ACTION = 3\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "\n",
    "# initialization of variables used in the main loop\n",
    "x_train, y_train, rewards = [],[],[]\n",
    "reward_sum = 0\n",
    "episode_nb = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './log' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "resume = True\n",
    "epochs_before_saving = 10\n",
    "\n",
    "# load pre-trained model if exist\n",
    "if (resume and os.path.isfile('my_model_weights.h5')):\n",
    "    print(\"loading previous weights\")\n",
    "    model.load_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    r = np.array(r)\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    # we go from last reward to first one so we don't have to do exponentiations\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        if r[t] != 0: running_add = 0 # if the game ended (in Pong), reset the reward sum\n",
    "        running_add = running_add * gamma + r[t] # the point here is to use Horner's method to compute those rewards efficiently\n",
    "        discounted_r[t] = running_add\n",
    "    discounted_r -= np.mean(discounted_r) #normalizing the result\n",
    "    discounted_r /= np.std(discounted_r) #idem\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At the end of episode 0 the total reward was : -21.0\n",
      "Epoch 1/1\n",
      "1091/1091 [==============================] - 0s 127us/step - loss: 0.0521 - accuracy: 0.9230\n",
      "At the end of episode 1 the total reward was : -21.0\n",
      "Epoch 1/1\n",
      "1112/1112 [==============================] - 0s 29us/step - loss: 0.0648 - accuracy: 0.9083\n",
      "At the end of episode 2 the total reward was : -21.0\n",
      "Epoch 1/1\n",
      "1103/1103 [==============================] - 0s 27us/step - loss: 0.0375 - accuracy: 0.9166\n",
      "At the end of episode 3 the total reward was : -21.0\n",
      "Epoch 1/1\n",
      "1180/1180 [==============================] - 0s 23us/step - loss: 0.0208 - accuracy: 0.9331\n",
      "At the end of episode 4 the total reward was : -21.0\n",
      "Epoch 1/1\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 0.0242 - accuracy: 0.9373\n",
      "At the end of episode 5 the total reward was : -21.0\n",
      "Epoch 1/1\n",
      "1111/1111 [==============================] - 0s 23us/step - loss: 0.0479 - accuracy: 0.9208\n",
      "At the end of episode 6 the total reward was : -21.0\n",
      "Epoch 1/1\n",
      "1101/1101 [==============================] - 0s 22us/step - loss: -0.0161 - accuracy: 0.9237\n",
      "At the end of episode 7 the total reward was : -21.0\n",
      "Epoch 1/1\n",
      "1106/1106 [==============================] - 0s 24us/step - loss: 0.0407 - accuracy: 0.9313\n",
      "At the end of episode 8 the total reward was : -21.0\n",
      "Epoch 1/1\n",
      "1104/1104 [==============================] - 0s 22us/step - loss: -0.0271 - accuracy: 0.8986\n",
      "At the end of episode 9 the total reward was : -21.0\n",
      "Epoch 1/1\n",
      "1091/1091 [==============================] - 0s 22us/step - loss: 0.0332 - accuracy: 0.9193\n",
      "At the end of episode 10 the total reward was : -21.0\n",
      "Epoch 1/1\n",
      "1088/1088 [==============================] - 0s 21us/step - loss: 0.0299 - accuracy: 0.9219\n",
      "At the end of episode 11 the total reward was : -21.0\n",
      "Epoch 1/1\n",
      "1093/1093 [==============================] - 0s 22us/step - loss: 0.0231 - accuracy: 0.9222\n",
      "At the end of episode 12 the total reward was : -21.0\n",
      "Epoch 1/1\n",
      "1102/1102 [==============================] - 0s 22us/step - loss: 0.0282 - accuracy: 0.9256\n",
      "At the end of episode 13 the total reward was : -21.0\n",
      "Epoch 1/1\n",
      "1187/1187 [==============================] - 0s 22us/step - loss: 0.0180 - accuracy: 0.9149\n",
      "At the end of episode 14 the total reward was : -21.0\n",
      "Epoch 1/1\n",
      "1247/1247 [==============================] - 0s 21us/step - loss: 0.0140 - accuracy: 0.9126\n",
      "At the end of episode 15 the total reward was : -21.0\n",
      "Epoch 1/1\n",
      "1108/1108 [==============================] - 0s 22us/step - loss: 0.0258 - accuracy: 0.9134\n",
      "At the end of episode 16 the total reward was : -21.0\n",
      "Epoch 1/1\n",
      "1185/1185 [==============================] - 0s 21us/step - loss: -0.0221 - accuracy: 0.9030\n",
      "At the end of episode 17 the total reward was : -21.0\n",
      "Epoch 1/1\n",
      "1111/1111 [==============================] - 0s 21us/step - loss: 0.0201 - accuracy: 0.9145\n",
      "At the end of episode 18 the total reward was : -21.0\n",
      "Epoch 1/1\n",
      "1176/1176 [==============================] - 0s 21us/step - loss: 0.0269 - accuracy: 0.9209\n",
      "At the end of episode 19 the total reward was : -21.0\n",
      "Epoch 1/1\n",
      "1110/1110 [==============================] - 0s 21us/step - loss: -0.0426 - accuracy: 0.9072\n",
      "At the end of episode 20 the total reward was : -21.0\n",
      "Epoch 1/1\n",
      "1088/1088 [==============================] - 0s 22us/step - loss: 0.0401 - accuracy: 0.9301\n",
      "At the end of episode 21 the total reward was : -21.0\n",
      "Epoch 1/1\n",
      "1096/1096 [==============================] - 0s 25us/step - loss: -0.0085 - accuracy: 0.9261\n",
      "At the end of episode 22 the total reward was : -21.0\n",
      "Epoch 1/1\n",
      "1109/1109 [==============================] - 0s 23us/step - loss: 0.0448 - accuracy: 0.9297\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fee1f852f2e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# do one step in our environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;31m#print('Observation: ' + str(observation))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m#print('Reward: ' + str(reward))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Kera/env_python_3_6/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Kera/env_python_3_6/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Kera/env_python_3_6/lib/python3.6/site-packages/numpy/core/_dtype.py\u001b[0m in \u001b[0;36m_name_get\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_name_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m     \u001b[0;31m# provides dtype.name.__get__, documented as returning a \"bit name\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main loop\n",
    "while (True):\n",
    "    ram = env.unwrapped._get_ram()\n",
    "    position_player_y = ram[ram_address_position_player_y]\n",
    "    position_ball_x = ram[ram_address_position_ball_x]\n",
    "    position_ball_y = ram[ram_address_position_ball_y]\n",
    "\n",
    "    # We will use as input an array of the difference between the current and previous positions.\n",
    "    cur_input = np.array([position_player_y, position_ball_x, position_ball_y])\n",
    "    #print('Current input: ' + str(cur_input))\n",
    "    #print('Prev input: ' + str(prev_input))\n",
    "    x = cur_input - prev_input if prev_input is not None else np.zeros(len(cur_input))\n",
    "    #print('Difference: ' + str(x))\n",
    "    prev_input = cur_input\n",
    "    \n",
    "    # forward the policy network and sample action according to the proba distribution\n",
    "    proba = model.predict(np.array([x]))\n",
    "    #print('Prediction: ' + str(proba))\n",
    "    # Variable proba is the probability prediction of how good UP_ACTION is for this frame.\n",
    "    # Then select UP_ACTION by proba percent. Easy way to still allow the other action.\n",
    "    # TODO: Mathematical reason for random number?\n",
    "    action = UP_ACTION if np.random.uniform() < proba else DOWN_ACTION\n",
    "    y = 1 if action == UP_ACTION else 0 # 0 and 1 are our labels\n",
    "\n",
    "    # log the input and label to train later\n",
    "    x_train.append(x)\n",
    "    y_train.append(y)\n",
    "\n",
    "    # do one step in our environment\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    #print('Observation: ' + str(observation))\n",
    "    #print('Reward: ' + str(reward))\n",
    "    #print('Done: ' + str(done))\n",
    "    #print('Info: ' + str(info))\n",
    "    rewards.append(reward)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    # end of an episode\n",
    "    if done:\n",
    "        print('At the end of episode', episode_nb, 'the total reward was :', reward_sum)\n",
    "        \n",
    "        # increment episode number\n",
    "        episode_nb += 1\n",
    "        \n",
    "        # training\n",
    "        # TODO: Is np.vstack is really necessary?\n",
    "        # TODO: Clarify sample_weight=discount_rewards(rewards, gamma)\n",
    "        model.fit(x=np.vstack(x_train), y=np.vstack(y_train), verbose=1, sample_weight=discount_rewards(rewards, gamma))\n",
    "                                                     \n",
    "        if episode_nb % epochs_before_saving == 0:    \n",
    "            model.save_weights('my_model_weights' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '.h5')\n",
    "            \n",
    "        # Reinitialization\n",
    "        x_train, y_train, rewards = [],[],[]\n",
    "        observation = env.reset()\n",
    "        reward_sum = 0\n",
    "        prev_input = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_python_3_6",
   "language": "python",
   "name": "env_python_3_6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
